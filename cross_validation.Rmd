---
title: "cross validation"
output: github_document
---

```{r}
library(tidyverse)

library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Fitting non-linear models - take his word for it

Model selection is hard - you have competing goals: 
1) fit your current dataset well 
2) AIC/ BIC make you pay a price for too much complexity 

If I've done a good job building my model on my first dataset, it should do a good job predicting a second dataset (cross-validation) 

Prediction accuracy - go into testing dataset, get fitted values based on my previous model, if i make good predictions, I'll have a small root mean squared error. We want small RMSE 

Typically you repeat the process MANY times 

Typically just does an 80/20 split and then do it 100 times and look at RMSE look like in each of those cases 

Can also compare models via cross validation or AIC/ BIC 

Prediction as a goal: you want to do cross-validation AFTER you've determined confounders, etc. 

Tools for CV: 
add_predictions add_residuals rmse crossv_mc 

## Simulate a dataset 

```{r}
set.seed(1)

nonlin_df = 
  tibble(
    id = 1:100, 
    x = runif(100, 0, 1), 
    y = 1-10 * (x-0.3) ^2 + rnorm(100, 0, 0.3) 
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

## Cross-validation by hand (Create splits by hand, plot, fit some models)

```{r}
train_df = sample_n(nonlin_df, 80)

test_df = anti_join(nonlin_df, train_df, by = "id")

train_df %>% arrange(id)
test_df %>% arrange(id)

ggplot(train_df, aes(x = x, y = y)) + 
  geom_point() +
  geom_point(data = test_df, color = "red")
```

for cross validation to work, you have to have a real/ full split between train and test dataset

## Fit my models with training dataset

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam( y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

plot the results

```{r}
train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_line(aes(y = pred))

train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_line(aes(y = pred))

train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_line(aes(y = pred))
```

The second one - putting too much stuff in the model 
Third one - not enough! 

## Quantify the results 

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

this function is going to take the linear model, given the coefficients in the linear model, I'm going to take the predictions from the testing df and find the rmse

This worked with one 80/20 split but maybe we try it again and again with different 80/20 splits 

## CV iteratively 

Use `modelr::crossv_md`

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100)
```

Comes up with a dataset with a resample object -training dataset, then a resample object - testing dataset, then the ID, and then we'll add our model outputs in following columns 

```{r}
cv_df %>% 
  pull(train) %>% 
  .[[1]] %>% 
  as_tibble()
```

We need to convert everything to a tible for mgcv to work 

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100) %>% 
  mutate(
    train = map(train, as_tibble), 
    test = map(test, as_tibble)
  )
```

Let's fit some models

```{r}
cv_df %>% 
  mutate(
    linear_mod = map(.x = train, ~lm(y ~ x, data = .x))
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(model = .x, data = .y))
  )

cv_df = 
  cv_df %>% 
  mutate(
    linear_mod = map(.x = train, ~lm(y ~ x, data = .x)), 
    smooth_mod = map(.x = train, ~gam(y ~ s(x), data = .x)),
    wiggly_mod = map(.x = train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(model = .x, data = .y)), 
    rmse_smooth = map2_dbl(.x = smooth_mod, .y = test, ~rmse(model = .x, data = .y)), 
    rmse_wiggly = map2_dbl(.x = wiggly_mod, .y = test, ~rmse(model = .x, data = .y))
  )
```

Everything relevant to the training and testing split is in this column here

Look at output 

```{r}
cv_df %>% 
  select(starts_with("rmse"))
```

I want to visualize the differences here, but this is not a tidy way to organize this dataset

```{r}
cv_df %>% 
  select(.id, starts_with("rmse")) %>% 
  pivot_longer(
    rmse_linear:rmse_wiggly,
    names_to = "model", 
    values_to = "rmse", 
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, ,y = rmse)) + 
  geom_boxplot()
  
```

## Child growth 

```{r}
child_growth = read_csv("./data/nepalese_children.csv") %>% 
  mutate(weight_cp = (weight > 7) * (weight - 7))
```

```{r}
child_growth %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .2)
```

People who have smaller arm circumfrances for higher weights 
Especially in the middle, maybe there's a linear association, but maybe at the beginning there's a trending downward situation

SHOULD WE consider a linear or non linear models? 

## Consider candidate models 

```{r}
linear_mod = lm(armc ~ weight, data = child_growth)
pwl_mod = lm(armc ~ weight + weight_cp, data = child_growth)
smooth_mod = gam(armc ~ s(weight), data = child_growth)
```

piecewise linear model - to get this to fit we have to add a variable (See code chunk above)

```{r}
child_growth %>% 
  add_predictions(pwl_mod) %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .2) + 
  geom_line(aes(y = pred), color = "red")

child_growth %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .2) + 
  geom_line(aes(y = pred), color = "red")

child_growth %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .2) + 
  geom_line(aes(y = pred), color = "red")
```

## Use CV to compare models 

```{r}
cv_df = 
  crossv_mc(child_growth, 100) %>% 
  mutate(
    train = map(train, as_tibble), 
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>% 
  mutate(
    linear_mod = map(.x = train, ~lm(armc ~ weight, data = .x)), 
    pwl_mod = map(.x = train, ~lm(armc ~ weight + weight_cp, data = .x)), 
    smooth_mod = map(.x = train, ~gam(armc ~ s(weight), data = .x))
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(model = .x, data = .y)), 
    rmse_pwl = map2_dbl(.x = pwl_mod, .y = test, ~rmse(model = .x, data = .y)), 
    rmse_smooth = map2_dbl(.x = smooth_mod, .y = test, ~rmse(model = .x, data = .y)), 
  )
```

Look at RMSE distributions

```{r}
cv_df %>% 
  select(.id, starts_with("rmse")) %>% 
  pivot_longer(
    rmse_linear:rmse_smooth, 
    names_to = "model", 
    values_to = "rmse", 
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_boxplot()
```

Between the smooth model and the pwl, which is the right one to choose? 

pwl is probably easier to interpret because of the straight line relationships here and here 

You can always test for a change point

Now, a smooth model is more acceptable 





